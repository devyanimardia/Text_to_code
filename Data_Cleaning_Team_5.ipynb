{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Code for data preprocessing"
      ],
      "metadata": {
        "id": "suOWhP_CpgsE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to handle missing values:"
      ],
      "metadata": {
        "id": "ltEs_4PolVyW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E9ckRPKpkcVo"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def handle_missing_values(json_file='your_json_file.json', output_file='cleaned_json.json'):\n",
        "    \"\"\"\n",
        "    Handles missing values in a JSON file with 'text' and 'code' fields, replacing null values with empty strings.\n",
        "\n",
        "    Args:\n",
        "    - json_file (str): The path to the input JSON file.\n",
        "    - output_file (str): The path to the output cleaned JSON file.\n",
        "\n",
        "    This function reads a JSON file, handles missing (null) values in the 'text' and 'code' fields, replacing null values\n",
        "    with empty strings, and writes the cleaned content to a new JSON file.\n",
        "    \"\"\"\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Handle missing values in 'text' and 'code' fields\n",
        "    for entry in data:\n",
        "        entry['text'] = entry.get('text', '') or ''\n",
        "        entry['code'] = entry.get('code', '') or ''\n",
        "\n",
        "    # Write cleaned data to a new JSON file\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(data, outfile, indent=4)\n",
        "\n",
        "# Calling the function with default file names\n",
        "handle_missing_values()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to remove other languages:"
      ],
      "metadata": {
        "id": "apIJUUeTluhm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "def process_json_data(input_file='data.json', output_file='filtered.json'):\n",
        "    \"\"\"\n",
        "    Loads JSON data from a file, processes it to filter out rows with Chinese characters,\n",
        "    and writes the filtered data to a new JSON file.\n",
        "\n",
        "    Args:\n",
        "    - input_file (str): The path to the input JSON file.\n",
        "    - output_file (str): The path to the output filtered JSON file.\n",
        "\n",
        "    This function loads JSON data from an input file into a DataFrame, creates a 'Has_Chinese'\n",
        "    column indicating the presence of Chinese characters in 'text' and 'code' columns,\n",
        "    filters rows with Chinese characters, and writes the resulting filtered DataFrame to a new JSON file.\n",
        "    \"\"\"\n",
        "    # Load JSON data into a DataFrame\n",
        "    with open(input_file, \"r\") as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Function to check for Chinese characters\n",
        "    def has_chinese(text):\n",
        "        return any('\\u4e00' <= char <= '\\u9fff' for char in text)\n",
        "\n",
        "    # Apply the function to check for Chinese characters in 'text' column\n",
        "    df['Has_Chinese'] = df['text'].apply(has_chinese)\n",
        "\n",
        "    # Filter rows where 'Has_Chinese' column is True\n",
        "    result = df[df['Has_Chinese']]\n",
        "\n",
        "    # Remove rows where 'Has_Chinese' is True\n",
        "    filtered_df = df[~df['Has_Chinese']]\n",
        "    filtered_df = filtered_df[['text', 'code']]\n",
        "\n",
        "    # Apply the function to check for Chinese characters in 'code' column\n",
        "    filtered_df['Has_Chinese'] = filtered_df['code'].apply(has_chinese)\n",
        "\n",
        "    # Filter rows where 'Has_Chinese' column is True\n",
        "    result = filtered_df[filtered_df['Has_Chinese']]\n",
        "    filtered_df = filtered_df[~filtered_df['Has_Chinese']]\n",
        "    filtered_df = filtered_df[['text', 'code']]\n",
        "\n",
        "    # Convert filtered DataFrame to JSON\n",
        "    json_output = filtered_df.to_json(orient='columns')\n",
        "    with open(output_file, 'w') as json_file:\n",
        "        json_file.write(json_output)\n",
        "\n",
        "# Calling the function with default file names\n",
        "process_json_data()\n"
      ],
      "metadata": {
        "id": "m1tq71twlvJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to standardize the data:"
      ],
      "metadata": {
        "id": "VEj174a6mbVU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def standardize_tabs(json_file='your_json_file.json', output_file='standardized_json.json'):\n",
        "    \"\"\"\n",
        "    Standardizes tab characters in 'text' and 'code' fields of a JSON file to '\\t'.\n",
        "\n",
        "    Args:\n",
        "    - json_file (str): The path to the input JSON file.\n",
        "    - output_file (str): The path to the output standardized JSON file.\n",
        "\n",
        "    This function reads a JSON file, standardizes tab characters in 'text' and 'code' fields to '\\t',\n",
        "    and writes the standardized content to a new JSON file.\n",
        "    \"\"\"\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Standardize tab characters in 'text' and 'code' fields to '\\t'\n",
        "    for entry in data:\n",
        "        entry['text'] = entry.get('text', '').replace('    ', '\\t')  # Replace 4 spaces with '\\t'\n",
        "        entry['code'] = entry.get('code', '').replace('    ', '\\t')  # Replace 4 spaces with '\\t'\n",
        "\n",
        "    # Write standardized data to a new JSON file\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(data, outfile, indent=4)\n",
        "\n",
        "# Calling the function with default file names\n",
        "standardize_tabs()\n"
      ],
      "metadata": {
        "id": "lG6Km46Qmb7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to standardize the line breaks in the data:"
      ],
      "metadata": {
        "id": "rOR5UNzhm0dI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def standardize_linebreaks(json_file='your_json_file.json', output_file='standardized_json.json'):\n",
        "    \"\"\"\n",
        "    Standardizes line breaks, dedent, and carriage returns in 'text' and 'code' fields of a JSON file.\n",
        "\n",
        "    Args:\n",
        "    - json_file (str): The path to the input JSON file.\n",
        "    - output_file (str): The path to the output standardized JSON file.\n",
        "\n",
        "    This function reads a JSON file, standardizes line breaks, dedent, and carriage returns in 'text' and 'code' fields,\n",
        "    and writes the standardized content to a new JSON file.\n",
        "    \"\"\"\n",
        "    with open(json_file, 'r') as file:\n",
        "        data = json.load(file)\n",
        "\n",
        "    # Standardize line breaks, dedent, and carriage returns in 'text' and 'code' fields\n",
        "    for entry in data:\n",
        "        entry['text'] = entry.get('text', '').replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
        "        entry['code'] = entry.get('code', '').replace('\\n', '\\\\n').replace('\\r', '\\\\r')\n",
        "\n",
        "    # Write standardized data to a new JSON file\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(data, outfile, indent=4)\n",
        "\n",
        "# Calling the function with default file names\n",
        "standardize_linebreaks()\n"
      ],
      "metadata": {
        "id": "73JC2YBYm1Fw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to merge all the json files:"
      ],
      "metadata": {
        "id": "0jKoXNNUoIcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import glob\n",
        "\n",
        "def merge_json_files(input_folder='folder_with_json_files', output_file='merged_file.json'):\n",
        "    \"\"\"\n",
        "    Merges multiple JSON files into a single JSON file.\n",
        "\n",
        "    Args:\n",
        "    - input_folder (str): The path to the folder containing JSON files to be merged.\n",
        "    - output_file (str): The path to the output merged JSON file.\n",
        "\n",
        "    This function reads all JSON files from a folder, merges their content into a single JSON object, and writes\n",
        "    the merged content to a new JSON file.\n",
        "    \"\"\"\n",
        "    # Fetch all JSON files in the folder\n",
        "    files = glob.glob(f\"{input_folder}/*.json\")\n",
        "\n",
        "    merged_data = []\n",
        "\n",
        "    # Read data from each file and append to the merged list\n",
        "    for file in files:\n",
        "        with open(file, 'r') as f:\n",
        "            data = json.load(f)\n",
        "            merged_data.append(data)\n",
        "\n",
        "    # Write merged data to a single JSON file\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        json.dump(merged_data, outfile)\n",
        "\n",
        "# Calling the function with default folder and output file names\n",
        "merge_json_files()"
      ],
      "metadata": {
        "id": "6rTH4zXqoI-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function to convert json to jsonl:"
      ],
      "metadata": {
        "id": "XnjsQhaboyTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def convert_json_to_jsonl(input_file='data.json', output_file='data.jsonl'):\n",
        "    \"\"\"\n",
        "    Converts a JSON file to a JSON lines file.\n",
        "\n",
        "    Args:\n",
        "    - input_file (str): The path to the input JSON file.\n",
        "    - output_file (str): The path to the output JSON lines file.\n",
        "\n",
        "    Converts each entry in the input JSON file to a separate line in the output JSON lines file.\n",
        "    \"\"\"\n",
        "    with open(input_file, 'r') as f:\n",
        "        json_data = json.load(f)\n",
        "\n",
        "    with open(output_file, 'w') as jsonl_output:\n",
        "        for entry in json_data:\n",
        "            json.dump(entry, jsonl_output)\n",
        "            jsonl_output.write('\\n')\n",
        "\n",
        "# Calling the function with default file names\n",
        "convert_json_to_jsonl()"
      ],
      "metadata": {
        "id": "tzKcHJqtoyoW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}